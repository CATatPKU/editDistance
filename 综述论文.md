## 词相似度计算（佳音）
### Damerau–Levenshtein distance（D氏距离）
#### 定义
- D氏距离是用来测量两个字符序列之间的编辑距离的字符串度量标准。两个词的Damerau-Levenshtein Distance是从一个词转换为另一个词的最少操作数，与Levenshtein Distance不同的是，除了单个字符的插入、删除和变更之外，还包括两个相邻字符的转换。[^1](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)
- 对于两个字符串a、b,函数![image](https://github.com/CATatPKU/editDistance/raw/master/images/1.png)
表示a的前i个字符与b的前j个字符的编辑距离:
![image](https://github.com/CATatPKU/editDistance/raw/master/images/2.png)
当![image](https://github.com/CATatPKU/editDistance/raw/master/images/3..png)时，除了要计算Levenshtein Distance中所定义的插入、删除和变更操作的操作数以外，还要计算相邻字符转换的操作数，然后将四个操作数做对比取最小的值。并且这个操作的代价（cost)也是1。                      
#### 计算原理
- optimal string alignment/restricted Damerau-Levenshtein distance（严格编辑距离）：
	不能往中间插入，每次只能更改串的最后一个字符（或者加入新字符或者删改原串中的最后一个字符.也就是说每次操作只能改变串A的一个位置（最后一个位置）。AC 和 CBA 的距离，若是严格编辑距离为4，则操作为：AC - CA - CB - CBA(或其他)；若是D氏距离则为3，操作为AC - CA - CBA,对于删除同理。
	代码区别举例：（增加了transposition操作）
``` 
    if i > 1 and j > 1 and a[i] = b[j-1] and a[i-1] = b[j] then
    d[i, j] := minimum(d[i, j],
                       d[i-2, j-2] + cost)  // transposition
```
- adjacent transpositions（交换相邻项算法）
	L氏距离的求解有个性质，就是某个步骤中的操作一旦被认为是最优的，则后续的操作不会再对前面的串再做更改（再更改会使总代价变高）。即存在最优子结构。D氏距离也存在最优子结构，即移项的代价大于等于插入和删除代价和一半时，移项交换过的元素不会再被更改。也就是只需要计算当前i，j最近的一次"交叉相等"即可.（即满足交叉相等的2个下标尽可能大）由于交换需要的条件是相邻，因此，该操作需要先删除串A的所有元素再插入所有B中相应的元素，移项的代价是![image](https://github.com/CATatPKU/editDistance/blob/master/images/4.png), 则删除的代价是至少一次插入和删除的代价平均值![image](https://github.com/CATatPKU/editDistance/blob/master/images/5.png).

### Longest Common Substring & Longest Common Subsequence （最长公共子序列与最长公共子串）
#### 定义
- 最长公共子串问题是寻找两个或多个已知字符串最长的子串。最长公共子序列是一个在一个序列集合中（通常为两个序列）用来查找所有序列中最长子序列的问题。
- 两者区别：子串要求在原字符串中是连续的，而子序列则只需保持相对顺序，并不要求连续。例如X = {P, Q, 3, 1}; Y = {P, 3, 1, d, f}那么，{P, 3, 1}是X和Y的最长公共子序列，但不是它们的最长公共字串。
#### 实例
  给定两个序列：X[1...m]和Y[1...n]，求在两个序列中同时出现的最长子序列的长度。假设 X 和 Y 的序列如下：
X[1...m] = {A, B, C, B, D, A, B}
Y[1...n] = {B, D, C, A, B, A}
可以看出，X 和 Y 的最长公共子序列有 “BDAB”、“BCAB”、“BCBA”，即长度为4。
#### 计算原理
- 当序列的数量确定时，问题可以使用动态规划在多项式时间内解决。而动态规划具有最优子结构和重叠子问题两个特性。问题具有最优子结构性质，我们才能写出最优解的递归方程；具有重叠子问题特性，我们才能通过避免重复计算来减少运行时间。[^2](http://www.geeksforgeeks.org/dynamic-programming-set-4-longest-common-subsequence/)


- 设 C[i,j] = |LCS(x[1...i],y[1...j])|，即C[i,j]表示序列X[1...i]和Y[1...j]的最长公共子序列的长度，则C[m,n] = |LCS(x,y)|就是问题的解。
递归推导式：![image](https://github.com/CATatPKU/editDistance/raw/master/images/6.png)
从这个递归公式可以看出，问题具有最优子结构性质。
根据上面的递归推导式，可以写出求LCS长度的递归伪代码：

``` 
LCS(x,y,i,j)
	if x[i] = y[j]
		then C[i,j] ← LCS(x,y,i-1,j-1)+1
		else C[i,j] ← max{LCS(x,y,i-1,j),LCS(x,y,i,j-1)}
	return C[i,j]
```
- 像这样使用简单的递归，在最坏情况下（X 和 Y 的所有字符都不匹配，即LCS的长度为0）的时间复杂度为 θ(2n)。这和穷举法一样还是指数级的，太慢了。
根据程序中 X 和 Y 的初始值，我们画出部分递归树：![image](https://github.com/CATatPKU/editDistance/raw/master/images/7.png)
递归树中红框标记的部分被调用了两次。如果画出完整的递归树，我们会看到很多重复的调用，所以这个问题具有重叠子问题的特性。
- 简单的递归之所以和穷举法一样慢，因为在递归过程中进行了大量的重复调用。而动态规划就是要解决这个问题，通过用一个表来保存子问题的结果，避免重复的计算，以空间换时间。前面我们已经证明，最长公共子序列问题具有动态规划所要求的两个特性，所以 LCS 问题可以用动态规划来求解。
下面是用动态规划（打表）解决LCS问题：
![image](https://github.com/CATatPKU/editDistance/raw/master/images/8.png)
  所以动态规划解决LCS问题的时间复杂度为θ(mn)，这比简单的递归实现要快多了。空间复杂度是θ(mn)，因为使用了一个动态规划表。
> 动态规划将原来具有指数级时间复杂度的搜索算法改进成了具有多项式时间复杂度的算法。其中的关键在于解决冗余（重复计算），这是动态规划算法的根本目的。
动态规划实质上是一种以空间换时间的技术，它在实现的过程中，不得不存储产生过程中的各种状态，所以它的空间复杂度要大于其它的算法。
综上所述，动态规划的关键是——记忆，空间换时间，不重复求解，从较小问题解逐步决策，构造较大问题的解。
## 编辑距离

## BK树
### 介绍
BK树（Burkhard Keller Tree）的概念最早由W.A.Burkhard和R.M. Keller于1972年提出。这种树结构可以根据Levenshtein编辑距离概念进行拼写检查、字符串模糊匹配。许多软件中的自动校正功能都可以基于BK树结构完成。[^1].

### 基本理论
BK树最早提出时是为了解决如何在文件中查找与查询关键码（key）最相近的key的问题。[^2]

**情景：**
假设有一文件X，其中所有的成员都已被编入索引，再假设有一个极大可能不在X文件中的查询单元q，应该采取什么样的办法在X中得到与q最相近的单元？
- Exahaustive search technique:对X中的每个元素逐一进行检查必然是一种可行方法。但当文件过大时，会在查询过程中耗费巨大时间成本。
- Binary search：使用二分法进行检索需要建立在X中文件的key可以进行排序的前提之上。
- BK树检索

**建树：**
bk树的建立基于Levenshtein编辑距离概念，定义d(x,y)为key x到key y的编辑距离。此外若有另一key z，则有如下性质成立：
- d(x,y)=0 #编辑距离为0，意为x=y
- d(x,y)=d(y,x)  #从x变到y的最少步数就是从y变到x的最少步数
- d(x,z)<=d(x,y)+d(y,z) #从x变到z所需的步数不会超过x先变成y再变成z的步数
其中第三点性质最为重要，由于和数学中的三角形不式（两边之和大于第三边）相似，被称作编辑距离中的三角不等式。

任意在X中选取元素x0作为根节点建树，基于d(x0,x)的值划分成X的子集X1,X2...而X1,X2...的节点将继续根据d(x00,x1),d(x10,x2)...的规律划分，更下层的节点通过同样的方法递归进行划分，直到X中所有元素被记录在以x0为根节点的BK树中。
![image](https://github.com/CATatPKU/editDistance/blob/master/images/clule1.PNG) BK树最早提出时的图例

### 应用
#### 模糊匹配与错误纠正
##### 模糊匹配
在查找最相近字符串的基础上，很快有人提出如何利用BK树进行模糊匹配的问题。[^3]

**情景：**
假设有一个字符集合S，一个长度相对较短的字符串模式q,希望返回所有与q最大编辑距离在k之内的所有结果（即所有d(q,x)<=k的结果）。
模糊字符串匹配最先出现于1992年，主要分为两种：
1. Word-oriented： 基于自然语言、信息获取，算法可以获取于q模式最接近的，精确到词层面的结果。
2. Sequence-oriented：此种匹配适用于并不是自然语言的文本，返回结果并非以词划分，而是字符序列。

此处讨论的是第一种基于词汇划分的模糊匹配：
- 传统使用动态规划方法，详细见前文有关动态规划内容。但这种方法有所局限：不适用于搜索文本过大，或者加入不确定性的搜索。
- BK树匹配：减少搜索次数，提高效率。

**实现：**
与常规词汇存储方法不同（一般采用倒排索引方法：单独存储文本中所有词汇以及词频），采用BK树方式任意挑选S集合中一个词汇a作为根节点建树，子树以不同整形值进行划分，并向下递归，直到S中所有的元素都包含在以a为根节点的树中。子节点i意为i节点中的元素与根节点a的编辑距离。
根据三角不等式，对于存储在i节点上的x来说有如下关系：
- d(a,q)-k<=i<=d(a,q)+k
![image](https://github.com/CATatPKU/editDistance/blob/master/images/clule2.png)

不符合如上关系的树枝将被丢弃，不再查找，因此大大减少了时间上的开销。继续从符合要求的节点向其子节点以同样的方式递归，最终稿返回所有S集合中符合d(q,x)<=k条件的x。

此外，这种搜索结构受词汇前缀的影响很小的同时对文本本身有较大的容错度，支持返回模糊匹配和精确匹配两种需求。[^3]

##### 错误纠正
传统动态规划的算法由于可以满足移位、加权等应用拥有很大的灵活性，但BK树有独特的适合它的应用场景，比如在错误查询和纠正上。
这种使用方法其实与模糊字符串匹配相近，将两个词的编辑距离看作“错误”（例如：编辑距离为0的两个词之间错误为0，编辑距离为2的两个词之间错误为2）。
BK树是多路查找树，并且是不规则的（但通常是平衡的）。BK树的查找时间一般以公式O(n*m*n)衡量，其中n代表字典中词汇总量，m代表正确词汇的平均大小，n为错误单词的长度。Ricardo Baeza-Yates的论文中阐释道，当错误为1时，搜索只需遍历全树的5-8%；错误为2时，需要遍历全树的17%-25%。[^3]如果要进行精确查找，也可以非常简单地将k设置为0进行。

### 简单实例
假设现在有一个字典dict，其中包含GAME,FAME,GAIN,AIM,几个元素，随机选择GAME作为根节点。插入单词FAME，根据Levenshtein编辑距离概念，它与GAME的距离为1，于是建立一个与GAME编辑距离为1的子节点。插入单词GAIN，它与GAME的距离为2，于是建立一个与GAME编辑距离为2的子节点。同样，为AIM建立一个编辑距离为3的子节点。[^5]

BK树中的每个节点都只有一个具有相同编辑距离的子节点。在发生冲突时，向下继续寻找，直到找到字符串节点的适当父节点。[^4]溜当插入GATE时，算得它与GAME距离也为1，与FAME相同，于是沿着那条编号为1的子节点向下查找，递归地插入到FAME所在子树。

下图为以GAME为根节点的，拥有FAME,GAIN,AIM,SAME,GATE,GAY,FRAME,ACM,HOME几个元素的BK树：
![image](https://github.com/CATatPKU/editDistance/blob/master/images/clule3bkGAME.gif)

**情景：**
假如输入单词GAIE，程序发现它不在字典dict中，需要返回字典中所有与GAIE距离为1的单词。

步骤：

1. BK树中所有的查询都必须从根节点开始。GAIE一词与根节点GAME的编辑距离为1，输出GAME。根据编辑距离三角不等式定理，此时只需在编辑距离范围在[1,2]（d(a,q)-k<=i<=d(a,q)+k）之间的节点查询并继续向下迭代即可。则此例不再寻找单词AIM所在分支以下的所有词汇。（由于以AIM为根的子树到GAME的编辑距离时3，而GAME和GAIE之间的编辑距离是1，那么AIM及其子树到GAIE的编辑距离至少都是2）此例字典中词汇较少，仅排除一支，但依旧可想当样本过大时，会裁剪掉相当大一部分的分支，减少计算次数。
2. 在符合条件的子节点上继续查找，方法与之前完全相同：1）判断父节点到查询词汇GAIE的编辑距离2）通过三角不等式进行排除3）输出编辑距离小于查询要求的结果（此处为1）4）直到所有符合要求节点遍历完成。
   
   例如继续计算与GAME编辑距离为2的GAIN线，父节点GAIN与查询单元GAIE的编辑距离为1，输出GAIN。根据三角不等式排除编辑距离再[1,2]之外的其他节点——这里FRAME分支被排除，继续沿编号为1或2的边递归下去……











[^1]: <https://www.geeksforgeeks.org/bk-tree-introduction-implementation/> 
[^2]: bk tree
[^3]: fast approxiate string
[^4]:<https://www.jianshu.com/p/cedbd94f4f45>
[^5]:<http://www.matrix67.com/blog/archives/333>

#### 实现（可能要加代码）



## 一个例子讲解

（要不代码加到这里）

## 句相似度计算（可欣）
### 相似度检测方法
用于计算两句子间语义相似度的方法非常广泛，下面是常见的几种方法。

#### 基准方法
  估计两句子间语义相似度最简单的方法就是求句子中所有单词词嵌入的平均值，然后计算两句子词嵌入之间的余弦相似性。很显然，这种简单的基准方法会带来很多变数。我们将研究，如果忽略终止词并用 TF-IDF 计算平均权重会带来怎样的影响。

#### 词移距离
  替代上述基准方法的其中一种有趣方法就是词移距离（Word Mover’s Distance）。词移距离使用两文本间的词嵌入，测量其中一文本中的单词在语义空间中移动到另一文本单词所需要的最短距离。
  
#### Smooth Inverse Frequency
  从语义上来讲，求一句话中词嵌入的平均值似乎给与不相关的单词太多权重了。而 Smooth Inverse Frequency 试着用两种方法解决这一问题：
加权：就像上文用的 TF-IDF，SIF取句中词嵌入的平均权重。每个词嵌入都由 a/(a + p(w)) 进行加权，其中 a 的值经常被设置为0.01，而 p(w) 是词语在语料中预计出现的频率。
  常见元素删除：接下来，SIF 计算了句子的嵌入中最重要的元素。然后它减去这些句子嵌入中的主要成分。这就可以删除与频率和句法有关的变量，他们和语义的联系不大。
  最后，SIF 使一些不重要的词语的权重下降，例如 but、just 等，同时保留对语义贡献较大的信息。

#### 预训练编码器
  预训练编码器的情况比较复杂。但是我们的结果显示编码器还不能完全利用训练的成果。谷歌的句子编码器看起来要比InferSent好一些，但是皮尔森相关系数的结果与SIF的差别不大。

#### 基于LSTM的语句相似度计算方法
Siamese Network 是指⽹络中包含两个或以上完全相同的⼦⽹络，多应 ⽤于语句相似度计算、⼈脸匹配、签名鉴别等任务上：
 ![](images/LSTM.png)
  以语句相似度计算为例，两边的⼦⽹络从 Embedding 层到 LSTM 层等 都是完全相同的，整个模型称作 MaLSTM（ManhaĴan LSTM）。
通过 LSTM 层的最后输出得到两句话的固定长度表⽰，再使⽤以下公式 计算两者的相似度，相似度在 0 ⾄ 1 之间。
 ![](images/W1.png)
下图是整个网络大致的过程，左右两个句子输入后，句子中的每个词对应一个数字，左右两句话分别映射成一个向量，各自经过一个LSTM网络抽取特征后，使用曼哈顿距离计算两边向量的差距，最终得出预测结果。
 ![](images/W2.png)
   
## 树结构 （Humi）
### 语言学中的句法学

#### 乔姆斯基的转换生成语法

   诺姆·乔姆斯基（Noam Chomsky）在20世纪50年代提出了转换生成语法（transformational-generative），在语言学领域掀起了一场革命。

   

   ##### 句子的表层结构和深层结构

   - 表层结构：一个句子实际表现出的句法形式
   - 深层结构：一个句子深层次的抽象的结构组合，包括决定该句子的结构取向的全部要素。

   

   ##### 转换生成语法的四大特点：

   - 转换生成语法应该能够生成语言中所有合乎句法规则的句子，而且只生成合乎句法规则的句子。

     就是所有句子都应满足转换生成语法。

   - 能够运用有限的规则生成无限的合乎规则的句子。

     这样，转换生成语法才能解释语言的创造性

   - 在生成句子时应该能够被重复使用，这称为循环性。

     解释语言的无穷性。

   - 还应能解释另外三种语言现象：

     - 为何有些表层结构差异很大句子却紧密联系？

       eg: 主动语态与被动语态的问题

       He broke the vase.

       The vase was broken by him.

       这两个句子的表层结构看起来非常不同，但是深层结构却是一样的。

     

     - 有些表层结构看起来相似句子，意思却大相径庭?

       eg：

       Tom is easy to please.

       Tom is eager to please.

       这两个句子的表层结构看起来几乎一样，但是仔细分析就会发现，第一个句子Tom是动作的承受者，而第二个句子中，Tom是动作的发出者，意思截然不同，深层结构也不同。

       

     - 需要解释结构歧义。

       eg:

       old woman and man

       old到底是修饰woman，然后和man并列还是old同时修饰woman和man？

   

   

#### 转换生成语法中的句子结构

   ##### 短语结构

     短语结构是一个句法概念。可由一个或一个以上的词组成，无论哪一类，都必须含词组名称所表示的词。

     - 名词词组（NP）：the pretty girl
     - 动词词组（VP）：often dream
     - 介词词组（PP）：very happy
     - 形容词词组（AP）：mainly about

     名词词组和动词词组是句子中最主要的短语结构。

   ##### 句子（the S rule)

     S--NP VP

![](images/1571908887461.png)


### 计算机自然语言处理中的句子处理
#### 语义层面的相似计算

以下选自论文：

Yuhua Li, Zuhair Bandar, David McLean and James O’Shea  A Method for Measuring Sentence Similarity and its Application to Conversational Agents 

句子由词组成，所以在比较句子相似性时，可以先将句子分词。

....

T1={w1, W2, ..., Wn},

T2={v1,v2,...，Vm}

那么1和2中出现的所有词的集合就是

T=TI U T2 = {q1, q2, ..., qk}

... 



eg: 

T1: RAM keeps things being worked with.

T2:The CPU uses RAM as a short-term memory store.

T={RAM keeps things being worked with The CPU uses as a short-term memory store }



词语语义向量用š表示

语义向量的每个条目对应单词集中的每个单词，因此维度相当于单词集中单词数量。

词语语义向量的值 ši(i=1,2,...,m), 由相似单词与句子中单词的语义相似度决定。

Case 1: If wi appears in the sentence, ši is set to 1. 
Case 2: If wi is not contained in T1, a semantic similarity score is computed between wi 
and each word in the sentence T1, 
Thus the most similar word in T1 to wi  is that with the highest similarity score 
ς.  If ς exceeds a preset threshold, then ši = ς, otherwise ši = 0


![](images/Humi_2.png)


#### 涉及到语序问题



Zhao Jingling，Zhang Huiyun, Cui Baojiang,Sentence Similarity Based on Semantic Vector Model 

有些句子所包含的词语完全相同，但是由于句子语序影响，表达出的意思完全不同。

T1：椅子的右边是一桌子；

T2：桌子的右边是一椅子；

T={椅子，的，右边，是，一，桌子}

我们可以根据字符在序列中的索引得出

r1={1,2,3,4,5,6}

r2={6,2,3,4,5,1}

![](images/Humi_3.png)

可以看出这两个句子所表达的意思是不一样的，但如果只从语义角度计算，那相似度就是一样的。因此在计算句子相似度时还需要考虑语序问题，目前查到的论文中提出了以下这种计算两个句子之间语序相似度的。

![](images/Humi_5.png)

#### 句子相似度整体计算

结合上面两个方面，句子相似度既要考虑到语义层面还要考虑到句子语序问题。

![](images/Humi_6.png)

T1:  My little boy loves baking cakes. 

T2:  All girls like to bake cakes. 

T3:  Girls like going to cake-bakes. 

T4:  Some boys enjoy baking cakes. 

T5:  When I was a boy I occasionally baked cakes. 

T6:  This little baked cake is inedible.

T7:  Skidding cars bake my cakes every time. 

T8:  On the little hill it was baking hot and the boy was caked in mud.

![](images/Humi_7.png)






## 树结构和翻译记忆的关系

