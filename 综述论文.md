## 词相似度计算（佳音）

## 树结构 （Humi）
### 语言学中的句法学

#### 乔姆斯基的转换生成语法

   诺姆·乔姆斯基（Noam Chomsky）在20世纪50年代提出了转换生成语法（transformational-generative），在语言学领域掀起了一场革命。

   

   ##### 句子的表层结构和深层结构

   - 表层结构：一个句子实际表现出的句法形式
   - 深层结构：一个句子深层次的抽象的结构组合，包括决定该句子的结构取向的全部要素。

   

   ##### 转换生成语法的四大特点：

   - 转换生成语法应该能够生成语言中所有合乎句法规则的句子，而且只生成合乎句法规则的句子。

     就是所有句子都应满足转换生成语法。

   - 能够运用有限的规则生成无限的合乎规则的句子。

     这样，转换生成语法才能解释语言的创造性

   - 在生成句子时应该能够被重复使用，这称为循环性。

     解释语言的无穷性。

   - 还应能解释另外三种语言现象：

     - 为何有些表层结构差异很大句子却紧密联系？

       eg: 主动语态与被动语态的问题

       He broke the vase.

       The vase was broken by him.

       这两个句子的表层结构看起来非常不同，但是深层结构却是一样的。

     

     - 有些表层结构看起来相似句子，意思却大相径庭?

       eg：

       Tom is easy to please.

       Tom is eager to please.

       这两个句子的表层结构看起来几乎一样，但是仔细分析就会发现，第一个句子Tom是动作的承受者，而第二个句子中，Tom是动作的发出者，意思截然不同，深层结构也不同。

       

     - 需要解释结构歧义。

       eg:

       old woman and man

       old到底是修饰woman，然后和man并列还是old同时修饰woman和man？

   

   

#### 转换生成语法中的句子结构

   ##### 短语结构

     短语结构是一个句法概念。可由一个或一个以上的词组成，无论哪一类，都必须含词组名称所表示的词。

     - 名词词组（NP）：the pretty girl
     - 动词词组（VP）：often dream
     - 介词词组（PP）：very happy
     - 形容词词组（AP）：mainly about

     名词词组和动词词组是句子中最主要的短语结构。

   ##### 句子（the S rule)

     S--NP VP

![](images/1571908887461.png)


### 计算机自然语言处理中的句子处理
#### 语义层面的相似计算

以下选自论文：

Yuhua Li, Zuhair Bandar, David McLean and James O’Shea  A Method for Measuring Sentence Similarity and its Application to Conversational Agents 

句子由词组成，所以在比较句子相似性时，可以先将句子分词。

....

T1={w1, W2, ..., Wn},

T2={v1,v2,...，Vm}

那么1和2中出现的所有词的集合就是

T=TI U T2 = {q1, q2, ..., qk}

... 



eg: 

T1: RAM keeps things being worked with.

T2:The CPU uses RAM as a short-term memory store.

T={RAM keeps things being worked with The CPU uses as a short-term memory store }



词语语义向量用š表示

语义向量的每个条目对应单词集中的每个单词，因此维度相当于单词集中单词数量。

词语语义向量的值 ši(i=1,2,...,m), 由相似单词与句子中单词的语义相似度决定。

Case 1: If wi appears in the sentence, ši is set to 1. 
Case 2: If wi is not contained in T1, a semantic similarity score is computed between wi 
and each word in the sentence T1, 
Thus the most similar word in T1 to wi  is that with the highest similarity score 
ς.  If ς exceeds a preset threshold, then ši = ς, otherwise ši = 0





#### 涉及到语序问题



Zhao Jingling，Zhang Huiyun, Cui Baojiang,Sentence Similarity Based on Semantic Vector Model 

有些句子所包含的词语完全相同，但是由于句子语序影响，表达出的意思完全不同。

T1：椅子的右边是一桌子；

T2：桌子的右边是一椅子；

T={椅子，的，右边，是，一，桌子}

我们可以根据字符在序列中的索引得出

r1={1,2,3,4,5,6}

r2={6,2,3,4,5,1}

![1571970260568](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571970260568.png)

可以看出这两个句子所表达的意思是不一样的，但如果只从语义角度计算，那相似度就是一样的。因此在计算句子相似度时还需要考虑语序问题，目前查到的论文中提出了以下这种计算两个句子之间语序相似度的。

![1571971896140](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571971896140.png)

#### 句子相似度整体计算

结合上面两个方面，句子相似度既要考虑到语义层面还要考虑到句子语序问题。

![1571977013486](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571977013486.png)

T1:  My little boy loves baking cakes. 

T2:  All girls like to bake cakes. 

T3:  Girls like going to cake-bakes. 

T4:  Some boys enjoy baking cakes. 

T5:  When I was a boy I occasionally baked cakes. 

T6:  This little baked cake is inedible.

T7:  Skidding cars bake my cakes every time. 

T8:  On the little hill it was baking hot and the boy was caked in mud.

![1571978515321](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1571978515321.png)
## 句相似度计算（可欣）
### 相似度检测方法
用于计算两句子间语义相似度的方法非常广泛，下面是常见的几种方法。

#### 基准方法
  估计两句子间语义相似度最简单的方法就是求句子中所有单词词嵌入的平均值，然后计算两句子词嵌入之间的余弦相似性。很显然，这种简单的基准方法会带来很多变数。我们将研究，如果忽略终止词并用 TF-IDF 计算平均权重会带来怎样的影响。

#### 词移距离
  替代上述基准方法的其中一种有趣方法就是词移距离（Word Mover’s Distance）。词移距离使用两文本间的词嵌入，测量其中一文本中的单词在语义空间中移动到另一文本单词所需要的最短距离。
  
#### Smooth Inverse Frequency
  从语义上来讲，求一句话中词嵌入的平均值似乎给与不相关的单词太多权重了。而 Smooth Inverse Frequency 试着用两种方法解决这一问题：
加权：就像上文用的 TF-IDF，SIF取句中词嵌入的平均权重。每个词嵌入都由 a/(a + p(w)) 进行加权，其中 a 的值经常被设置为0.01，而 p(w) 是词语在语料中预计出现的频率。
  常见元素删除：接下来，SIF 计算了句子的嵌入中最重要的元素。然后它减去这些句子嵌入中的主要成分。这就可以删除与频率和句法有关的变量，他们和语义的联系不大。
  最后，SIF 使一些不重要的词语的权重下降，例如 but、just 等，同时保留对语义贡献较大的信息。

#### 预训练编码器
  预训练编码器的情况比较复杂。但是我们的结果显示编码器还不能完全利用训练的成果。谷歌的句子编码器看起来要比InferSent好一些，但是皮尔森相关系数的结果与SIF的差别不大。


## 编辑距离

## BK树
### 介绍
BK树（Burkhard Keller Tree）的概念最早由W.A.Burkhard和R.M. Keller于1972年提出。这种树结构可以根据Levenshtein编辑距离概念进行拼写检查、字符串模糊匹配。许多软件中的自动校正功能都可以基于BK树结构完成。[^1].

### 基本理论
BK树最早提出时是为了解决如何在文件中查找与查询关键码（key）最相近的key的问题。[^2]

**情景：**
假设有一文件X，其中所有的成员都已被编入索引，再假设有一个极大可能不在X文件中的查询单元q，应该采取什么样的办法在X中得到与q最相近的单元？
- Exahaustive search technique:对X中的每个元素逐一进行检查必然是一种可行方法。但当文件过大时，会在查询过程中耗费巨大时间成本。
- Binary search：使用二分法进行检索需要建立在X中文件的key可以进行排序的前提之上。
- BK树检索

**建树：**
bk树的建立基于Levenshtein编辑距离概念，定义d(x,y)为key x到key y的编辑距离。此外若有另一key z，则有如下性质成立：
- d(x,y)=0 #编辑距离为0，意为x=y
- d(x,y)=d(y,x)  #从x变到y的最少步数就是从y变到x的最少步数
- d(x,z)<=d(x,y)+d(y,z) #从x变到z所需的步数不会超过x先变成y再变成z的步数
其中第三点性质最为重要，由于和数学中的三角形不式（两边之和大于第三边）相似，被称作编辑距离中的三角不等式。

任意在X中选取元素x0作为根节点建树，基于d(x0,x)的值划分成X的子集X1,X2...而X1,X2...的节点将继续根据d(x00,x1),d(x10,x2)...的规律划分，更下层的节点通过同样的方法递归进行划分，直到X中所有元素被记录在以x0为根节点的BK树中。
**【插图1】** BK树最早提出时的图例

### 应用
#### 模糊匹配与错误纠正
##### 模糊匹配
在查找最相近字符串的基础上，很快有人提出如何利用BK树进行模糊匹配的问题。[^3]

**情景：**
假设有一个字符集合S，一个长度相对较短的字符串模式q,希望返回所有与q最大编辑距离在k之内的所有结果（即所有d(q,x)<=k的结果）。
模糊字符串匹配最先出现于1992年，主要分为两种：
1. Word-oriented： 基于自然语言、信息获取，算法可以获取于q模式最接近的，精确到词层面的结果。
2. Sequence-oriented：此种匹配适用于并不是自然语言的文本，返回结果并非以词划分，而是字符序列。

此处讨论的是第一种基于词汇划分的模糊匹配：
- 传统使用动态规划方法，详细见前文有关动态规划内容。但这种方法有所局限：不适用于搜索文本过大，或者加入不确定性的搜索。
- BK树匹配：减少搜索次数，提高效率。

**实现：**
与常规词汇存储方法不同（一般采用倒排索引方法：单独存储文本中所有词汇以及词频），采用BK树方式任意挑选S集合中一个词汇a作为根节点建树，子树以不同整形值进行划分，并向下递归，直到S中所有的元素都包含在以a为根节点的树中。子节点i意为i节点中的元素与根节点a的编辑距离。
根据三角不等式，对于存储在i节点上的x来说有如下关系：
- d(a,q)-k<=i<=d(a,q)+k
**【图片2】**

不符合如上关系的树枝将被丢弃，不再查找，因此大大减少了时间上的开销。继续从符合要求的节点向其子节点以同样的方式递归，最终稿返回所有S集合中符合d(q,x)<=k条件的x。

此外，这种搜索结构受词汇前缀的影响很小的同时对文本本身有较大的容错度，支持返回模糊匹配和精确匹配两种需求。[^3]

##### 错误纠正
传统动态规划的算法由于可以满足移位、加权等应用拥有很大的灵活性，但BK树有独特的适合它的应用场景，比如在错误查询和纠正上。
这种使用方法其实与模糊字符串匹配相近，将两个词的编辑距离看作“错误”（例如：编辑距离为0的两个词之间错误为0，编辑距离为2的两个词之间错误为2）。
BK树是多路查找树，并且是不规则的（但通常是平衡的）。BK树的查找时间一般以公式O(n*m*n)衡量，其中n代表字典中词汇总量，m代表正确词汇的平均大小，n为错误单词的长度。Ricardo Baeza-Yates的论文中阐释道，当错误为1时，搜索只需遍历全树的5-8%；错误为2时，需要遍历全树的17%-25%。[^3]如果要进行精确查找，也可以非常简单地将k设置为0进行。

### 简单实例
假设现在有一个字典dict，其中包含GAME,FAME,GAIN,AIM,几个元素，随机选择GAME作为根节点。插入单词FAME，根据Levenshtein编辑距离概念，它与GAME的距离为1，于是建立一个与GAME编辑距离为1的子节点。插入单词GAIN，它与GAME的距离为2，于是建立一个与GAME编辑距离为2的子节点。同样，为AIM建立一个编辑距离为3的子节点。[^5]

BK树中的每个节点都只有一个具有相同编辑距离的子节点。在发生冲突时，向下继续寻找，直到找到字符串节点的适当父节点。[^4]溜当插入GATE时，算得它与GAME距离也为1，与FAME相同，于是沿着那条编号为1的子节点向下查找，递归地插入到FAME所在子树。

下图为以GAME为根节点的，拥有FAME,GAIN,AIM,SAME,GATE,GAY,FRAME,ACM,HOME几个元素的BK树：【图bkGAME】

**情景：**
假如输入单词GAIE，程序发现它不在字典dict中，需要返回字典中所有与GAIE距离为1的单词。

步骤：

1. BK树中所有的查询都必须从根节点开始。GAIE一词与根节点GAME的编辑距离为1，输出GAME。根据编辑距离三角不等式定理，此时只需在编辑距离范围在[1,2]（d(a,q)-k<=i<=d(a,q)+k）之间的节点查询并继续向下迭代即可。则此例不再寻找单词AIM所在分支以下的所有词汇。（由于以AIM为根的子树到GAME的编辑距离时3，而GAME和GAIE之间的编辑距离是1，那么AIM及其子树到GAIE的编辑距离至少都是2）此例字典中词汇较少，仅排除一支，但依旧可想当样本过大时，会裁剪掉相当大一部分的分支，减少计算次数。
2. 在符合条件的子节点上继续查找，方法与之前完全相同：1）判断父节点到查询词汇GAIE的编辑距离2）通过三角不等式进行排除3）输出编辑距离小于查询要求的结果（此处为1）4）直到所有符合要求节点遍历完成。
   
   例如继续计算与GAME编辑距离为2的GAIN线，父节点GAIN与查询单元GAIE的编辑距离为1，输出GAIN。根据三角不等式排除编辑距离再[1,2]之外的其他节点——这里FRAME分支被排除，继续沿编号为1或2的边递归下去……











[^1]: <https://www.geeksforgeeks.org/bk-tree-introduction-implementation/> 
[^2]: bk tree
[^3]: fast approxiate string
[^4]:<https://www.jianshu.com/p/cedbd94f4f45>
[^5]:<http://www.matrix67.com/blog/archives/333>

#### 实现（可能要加代码）



## 一个例子讲解

（要不代码加到这里）

## 树结构和翻译记忆的关系
